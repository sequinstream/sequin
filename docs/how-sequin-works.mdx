---
title: 'How Sequin works'
description: Stream tables with Sequin in real-time
icon: "wrench"
iconType: "solid"
---

With Sequin, you can stream Postgres by creating [**sequences**](#sequences). Sequences present a strictly ordered view of rows from one or more tables. Then, you can use this sequence to consume a table's rows by:

1. Using the [Consume API](#consume-api)
2. [Receiving webhooks](#webhooks)
3. Using the [Sync API](#sync-api) (coming soon)

Whether you're pulling rows or having them pushed to you, Sequin always delivers the latest version of rows in order. When new rows are inserted or when rows are updated, they're re-delivered to you.

If you want to capture discrete change events to rows, you can use a [WAL Pipeline](#wal-pipeline). With a WAL Pipeline, Sequin will capture all inserts, updates, and deletes along with `new` and `old` values, and store them in an event log table in your database. You can then create a sequence to consume the rows in your event table.

You can also filter and transform rows before they're delivered to your application:

<Frame>
  <img src="/images/core/consumer-workflow-diagram.png" alt="Diagram of data flowing from the source table, through filters and transforms, to the consumer for processing."  />
</Frame>

## Sequences

Sequin produces a **sequence** for every table you connect. Due to Postgres' multiversion concurrency control, tables do not provide strict ordering guarantees [[1](https://blog.sequinstream.com/postgres-sequences-can-commit-out-of-order/)]. This makes them difficult to stream properly without skipping rows [[2](https://blog.sequinstream.com/build-your-own-sqs-or-kafka-with-postgres/)].

To produce a stricly ordered sequence, Sequin uses a combination of the source table and a replication slot to produce a sequence. This makes it possible to [consume rows](#consuming-rows) from a table. When rows are inserted or updated, Sequin adds the rows to the end of the sequence so consumers will process them. Sequin stores sequence information in its internal catalog tables.

When you setup a sequence for a table in Sequin, you must specify its **sort column**. Sequin will use this sort column to produce the initial sequence. You can also use the sort column when rewinding a consumer, as it approximates the sequence's order.

<Note>
The sort column orders historical rows in the sequence. For new and updated rows, their order in the sequence will depend on when they are committed. This is because auto-increment and datetime columns will [not necessarily commit in order in Postgres](https://blog.sequinstream.com/postgres-sequences-can-commit-out-of-order).

After the sequence starts, the order of rows in the sequence will be the order they appear in Sequin's replication slot. This corresponds to the latest entry of the row in the WAL (i.e. the most recent [LSN](https://www.postgresql.org/docs/current/datatype-pg-lsn.html) corresponding to a given row).
</Note>

It's important you choose the right sort column. The right sort column is either:

- A **timestamp** like `updated_at` that your system updates whenever a row is inserted or updated.
- A **Postgres sequence** like `idx` or `index` that your system increments whenever a row is inserted or updated.

If your table does not have a timestamp or integer column that is updated on insert/update, you can add one:

<AccordionGroup>
  <Accordion icon="clock" title="Adding an updated_at column">
    Here's how you can add an `updated_at` column to your table and create a trigger that updates it on changes:

    ```sql
    -- Add the updated_at column
    alter table your_table add column updated_at timestamp default now();

    -- Update existing rows (if needed)
    -- This will add *some* sort to existing rows, which may be desirable.
    -- (Note if your table has millions of rows, this could take a while)
    update your_table set updated_at = inserted_at;

    -- Create a function to update the timestamp
    create or replace function update_timestamp()
    returns trigger as $$
    begin
      new.updated_at = now();
      return new;
    end;
    $$ language plpgsql;

    -- Create a trigger to call the function
    create trigger set_timestamp
    before update on your_table
    for each row
    execute function update_timestamp();
    ```

    This will ensure that the `updated_at` column is set to the current time on insert and updated whenever a row is modified.
  </Accordion>

  <Accordion icon="list-ol" title="Adding a sequence column">
    Here's how you can add an auto-incrementing `idx` column to your table:

    ```sql
    -- Create a sequence
    create sequence your_table_idx_seq;

    -- Add the idx column
    alter table your_table add column idx integer default nextval('your_table_idx_seq');

    -- Update existing rows (if needed)
    -- (Note if your table has millions of rows, this could take a while)
    update your_table set idx = nextval('your_table_idx_seq');
    ```

    This will ensure that the `idx` column is automatically incremented for new rows. For updates, you'll add this trigger:

    ```sql
    create or replace function update_idx()
    returns trigger as $$
    begin
      new.idx = nextval('your_table_idx_seq');
      return new;
    end;
    $$ language plpgsql;

    create trigger set_idx
    before update on your_table
    for each row
    execute function update_idx();
    ```

    This trigger will ensure the `idx` is updated on row updates.
  </Accordion>
</AccordionGroup>

<Note>If your table is append-only, you can use `inserted_at` or an auto-incrementing `id` column as the sort column. `uuid` columns will not work as they are not sequential.</Note>

## Rows

Sequin converts Postgres rows into JSON objects that are then delivered to your consumers.

A row has this shape:

```js
{
  record: {
    [key: string]: any;
  };
  metadata: {
    table_schema: string;
    table_name: string;
    consumer: {
      id: string;
      name: string;
    };
  };
}
```

For example:

```json
{
  "record": {
    "id": 1,
    "name": "Paul Atreides",
    "title": "Duke of Arrakis",
    "spice_allocation": 1000,
    "is_kwisatz_haderach": true
  },
  "metadata": {
    "table_schema": "public",
    "table_name": "house_atreides_members",
    "consumer": {
      "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
      "name": "dune_characters_consumer"
    }
  }
}
```

## Consuming rows

After creating a sequence, you have a few options for consuming rows with Sequin:

### Consume API

With the **Consume API**, you can setup an HTTP endpoint that your apps and services pull rows from. The endpoint follows the popular **consumer group** pattern of other queues and streams:

1. You can have many **consumers** (processes or workers) all pulling from the consume endpoint at the same time.
2. After a batch of rows is delivered to a consumer in the group, that consumer has a certain period of time to process the rows. This **visibility timeout** is configurable at the group level.
3. After processing a batch of rows, the consumer acknowledges them by calling the [acknowledge endpoint](/management-api/http-pull-consumers/ack).
4. If the consumer fails to process the batch, rows in the batch will be made available again for delivery to other consumers in the group.
5. When a row is updated in the database, it is re-delivered to the group.

You can setup many consumer groups for a single table.

[Read more](/management-api/http-pull-consumers/overview) about the Consume API endpoints.

<Frame>
  <img src="/images/core/pull-instructions.png" alt="The Sequin UI, displaying instructions for pulling messages from a consumer. The instructions are curl requests to the consumer's endpoint." style={{ maxWidth: '500px' }} />
</Frame>

### Webhooks

Sequin's webhook subscriptions are powered by the same consumer group system as the [Consume API](#consume-api). Each webhook payload contains one row. Rows are processed exactly once per subscription.

If your webhook endpoint returns a non-200 status code or fails to process the webhook within a configurable timeout, the row will be re-delivered.

Sequin will backoff and retry webhook delivery indefinitely.

<Frame>
  <img src="/images/core/http-endpoint-config.png" alt="The Sequin UI, prompting you to specify the HTTP endpoint to send messages to." style={{ maxWidth: '500px' }} />
</Frame>

You can monitor webhook status on the Sequin dashboard, including failing and outstanding messages.

{/* todo: image here */}

### Sync API

<Note>
The sync API is still in development.
</Note>

The **Sync API** is a simple alternative to the [Consume API](#consume-api). With the Sync API, Sequin presents your table as a paginateable endpoint. You can start paginating from any point in the table. When you reach the end, you can long-poll to receive new and updated rows in real-time.

The Sync API is great for situations where you want to manage the state of the cursor and don't need multiple consumers to process a stream in parallel.

## Filtering

In all of Sequin's consuming paradigms, you can specify one or more **filters** to process a subset of a table's rows or changes. The filters support SQL operators:

<Frame>
  <img src="/images/core/filters-config.png" alt="The Sequin UI, prompting you to specify the filters for a consumer. There is a list of columns to choose from, a list of operators, and an input field for values." style={{ maxWidth: '500px' }} />
</Frame>

### Transforming

<Note>
Transforms are still under development and coming soon.
</Note>

You can **transform** messages before they're delivered to your consumers using a Lua function. This is useful for transforming data into a format more suitable for your application.

## WAL Pipelines

Sequin streams rows directly from your tables. But sometimes, you want to log every discrete change to rows, and capture the `old` and `new` values of that change. Or, you want to retain a log of deleted rows.

With **WAL Pipelines**, Sequin captures changes from tables you specify. Sequin will write each change as it happens to a table in your database. Then, you can setup a sequence for your event table to stream its rows.

WAL Pipelines can indicate which operations to capture (i.e. `insert`, `update`, `delete`) as well as apply SQL filtering on changes.

If you don't want to keep every change indefinitely, you can setup your target event table with a retention policy using extensions such as `pg_cron` or `pg_partman`. By retaining recent changes in a Postgres table, you get to use Sequin features like replay and rewind.

Event tables look like this:

```sql
create table your_event_log (
  id serial primary key,
  seq bigint not null,
  source_database_id uuid not null,
  source_table_oid bigint not null,
  source_table_schema text not null,
  source_table_name text not null,
  record_pk text not null,
  record jsonb not null,
  changes jsonb,
  action text not null,
  committed_at timestamp with time zone not null,
  inserted_at timestamp with time zone not null default now()
);
```

<ResponseField name="id" type="integer" required>
  Auto-generated, auto-incrementing ID for the event entry.
</ResponseField>

<ResponseField name="seq" type="bigint" required>
  (internal) A Sequin sequence for the event entry.
</ResponseField>

<ResponseField name="source_database_id" type="uuid" required>
  (internal) The Sequin ID for your source database.
</ResponseField>

<ResponseField name="source_table_oid" type="bigint" required>
  The OID for the source table.
</ResponseField>

<ResponseField name="source_table_schema" type="text" required>
  The schema of the source table.
</ResponseField>

<ResponseField name="source_table_name" type="text" required>
  The name of the source table.
</ResponseField>

<ResponseField name="record_pk" type="text" required>
  The primary key for the source row. It's `text`, regardless of the type of the source's primary key type.
</ResponseField>

<ResponseField name="record" type="jsonb" required>
  For inserts and updates, this contains all the latest field values for the row (i.e. `new`). For deletes, this contains all the field values prior to deletion (i.e. `old`).
</ResponseField>

<ResponseField name="changes" type="jsonb">
  For updates, this is a JSON of all the `old` fields _that changed_ in this update. `changes` does not include unchanged values. So, to get the entire `old` row, just merge `changes` on top of `record`. (`null` for insert and delete operations.)
</ResponseField>

<ResponseField name="action" type="text" required>
  One of `insert`, `update`, or `delete`.
</ResponseField>

<ResponseField name="committed_at" type="timestamp with time zone" required>
  The time the change was committed.
</ResponseField>

<ResponseField name="inserted_at" type="timestamp with time zone" required>
  The time this event was inserted into the event table.
</ResponseField>

When setting up a WAL Pipeline, Sequin will walk you through the full instructions for creating the table. You can also view those instructions here:

<AccordionGroup>
  <Accordion icon="log" title="Creating a destination table for a WAL Pipeline">
    ```sql
      create table your_event_log (
        id serial primary key,
        seq bigint not null,
        source_database_id uuid not null,
        source_table_oid bigint not null,
        source_table_schema text not null,
        source_table_name text not null,
        record_pk text not null,
        record jsonb not null,
        changes jsonb,
        action text not null,
        committed_at timestamp with time zone not null,
        inserted_at timestamp with time zone not null default now()
      );

      create unique index on your_event_log (source_database_id, seq, record_pk);
      create index on your_event_log (seq);
      create index on your_event_log (source_table_oid);
      create index on your_event_log (committed_at);

      -- important comment to identify this table as a sequin events table
      comment on table your_event_log is '$sequin-events$';
    ```

    If you want to setup a retention policy on your event table with `pg_partman`, you can do so like this:

    ```sql
      -- create required extensions
      create extension if not exists pg_partman;
      create extension if not exists pg_cron;

      -- set up pg_partman for time-based partitioning
      select partman.create_parent(
        p_parent_table => 'your_event_table',
        p_control => 'committed_at',
        p_type => 'native',
        p_interval => 'daily',
        p_premake => 30
      );

      -- set up retention policy
      select partman.add_to_part_config(
        p_parent_table => 'your_event_table',
        p_retention => '${retentionDays} days',
        p_retention_keep_table => false
      );

      -- setup cron job to run maintenance for pg_partman every hour
      -- this is necessary to clean up old partitions (i.e. drop stale data)
      select cron.schedule('partman_maintenance', '0 * * * *', $$
        select partman.run_maintenance(p_analyze := false);
      $$);
    ```
  </Accordion>
</AccordionGroup>

## Replication slots

To maintain sequences, Sequin needs to detect changes in your database. Core to that process is a [replication slot](https://www.postgresql.org/docs/9.4/catalog-pg-replication-slots.html) and a corresponding [publication](https://www.postgresql.org/docs/current/logical-replication-publication.html). The publication determines which tables are captured by the replication slot.

Replication slots ensure Sequin never misses a change, even if Sequin is disconnected from your database for a period of time.

## Guarantees

### Strict ordering

Sequin guarantees that messages with the same primary key are processed in order for a given consumer.

Sync endpoints are strictly ordered. For the Consume API and webhooks, Sequin will not deliver messages to a consumer until the previous messages for that primary key has been acknowledged.

This is useful for applications that need to process changes to a single record in order. For example, if you're processing changes to a user's profile, you want to make sure that the changes are applied in order.

### Exactly-one processing

Both the Consume API and webhooks have exactly-once processing guarantees.