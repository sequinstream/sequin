---
title: 'How Sequin works'
description: Stream changes from Postgres with Sequin in real-time
icon: "wrench"
iconType: "solid"
---

{/* TODO: IGNORE THIS DOC, it is in-flight/mid-refactor */}

Sequin captures changes from your Postgres database and delivers them to sinks like Kafka, SQS, Redis, or HTTP endpoints. You can:

1. Stream [changes and rows](#message-shapes) in real-time using [logical replication](#replication-slots)
2. [Backfill existing rows](#backfills) to sinks
3. [Filter](#filtering) which changes get delivered
4. [Transform](#transforming) changes before delivery (coming soon)

## Message shapes

Sequin converts Postgres changes and rows into JSON messages that are delivered to your sinks.

When you setup a sink, you can choose whether the sink should receive [change messages](#change-messages) or [row messages](#row-messages).

### Change messages

A change message has this shape:

```js
{
  record: {
    [key: string]: any;  // The row data
  };
  changes: null | {
    [key: string]: any;  // Previous values for changed fields (updates only)
  };
  action: "insert" | "update" | "delete" | "read";
  metadata: {
    table_schema: string;
    table_name: string;
    commit_timestamp: string;  // ISO timestamp
    sink: {
      id: string;
      name: string;
    };
  };
}
```

A `read` operation occurs during [backfills](#backfills) when Sequin reads existing rows from your table. Unlike `insert`, `update`, or `delete` operations which happen in real-time, `read` operations represent historical data being loaded into your sink.

<AccordionGroup>
  <Accordion title="Insert Example" defaultOpen>
    ```json
    {
      "record": {
        "id": 1234,
        "customer_id": 789,
        "product": "Wireless Headphones",
        "quantity": 1,
        "price": 199.99,
        "status": "pending",
        "created_at": "2024-10-28T21:37:53",
        "updated_at": "2024-10-28T21:37:53"
      },
      "changes": null,
      "action": "insert",
      "metadata": {
        "table_schema": "public",
        "table_name": "orders",
        "commit_timestamp": "2024-10-28T21:37:53Z",
        "sink": {
          "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
          "name": "orders_kafka"
        }
      }
    }
    ```
  </Accordion>

  <Accordion title="Update Example">
    ```json
    {
      "record": {
        "id": 1234,
        "customer_id": 789,
        "product": "Wireless Headphones",
        "quantity": 1,
        "price": 199.99,
        "status": "shipped",
        "created_at": "2024-10-28T21:37:53",
        "updated_at": "2024-10-28T21:39:21"
      },
      "changes": {
        "status": "pending",
        "updated_at": "2024-10-28T21:37:53"
      },
      "action": "update",
      "metadata": {
        "table_schema": "public",
        "table_name": "orders",
        "commit_timestamp": "2024-10-28T21:39:21Z",
        "sink": {
          "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
          "name": "orders_kafka"
        }
      }
    }
    ```
  </Accordion>

  <Accordion title="Delete Example">
    ```json
    {
      "record": {
        "id": 1234,
        "customer_id": 789,
        "product": "Wireless Headphones",
        "quantity": 1,
        "price": 199.99,
        "status": "shipped",
        "created_at": "2024-10-28T21:37:53",
        "updated_at": "2024-10-28T21:39:21"
      },
      "changes": null,
      "action": "delete",
      "metadata": {
        "table_schema": "public",
        "table_name": "orders",
        "commit_timestamp": "2024-10-28T21:40:00Z",
        "sink": {
          "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
          "name": "orders_kafka"
        }
      }
    }
    ```
  </Accordion>

  <Accordion title="Read Example (Backfill)">
    ```json
    {
      "record": {
        "id": 1234,
        "customer_id": 789,
        "product": "Wireless Headphones",
        "quantity": 1,
        "price": 199.99,
        "status": "shipped",
        "created_at": "2024-10-28T21:37:53",
        "updated_at": "2024-10-28T21:39:21"
      },
      "changes": null,
      "action": "read",
      "metadata": {
        "table_schema": "public",
        "table_name": "orders",
        "commit_timestamp": "2024-10-28T21:39:21Z",
        "sink": {
          "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
          "name": "orders_kafka"
        }
      }
    }
    ```
  </Accordion>
</AccordionGroup>

For update operations, `record` contains the new values and `changes` contains the previous values of only the fields that changed. For all other operations, `changes` is null.

### Row messages

A row message has this shape:

```js
{
  record: {
    [key: string]: any;
  };
  metadata: {
    table_schema: string;
    table_name: string;
    sink: {
      id: string;
      name: string;
    };
  };
}
```

For example:

```json
{
  "record": {
    "id": 1,
    "name": "Paul Atreides",
    "title": "Duke of Arrakis",
    "spice_allocation": 1000,
    "is_kwisatz_haderach": true
  },
  "metadata": {
    "table_schema": "public", 
    "table_name": "house_atreides_members",
    "sink": {
      "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
      "name": "dune_characters_kafka"
    }
  }
}
```

<Info>
  At the moment, row messages only support `insert` and `update` operations. Soon, we'll support `delete` operations by adding a `deleted` boolean column to the message.
</Info>

### Which message shape should you use?

Change messages are the best fit for event-driven workflows. Often, events need to know the operation on the row (i.e. insert vs update) and need to know precisely what changed.

For replication use cases, row messages might be easier to work with. That's because every message is the same shape, whether it's an insert, update, or backfill ("read") message.

## Streams

Sequin produces a **Stream** for every table you connect. Due to Postgres' multiversion concurrency control, tables do not provide strict ordering guarantees [¹](https://blog.sequinstream.com/postgres-sequences-can-commit-out-of-order/). This makes them difficult to stream properly without skipping rows [²](https://blog.sequinstream.com/build-your-own-sqs-or-kafka-with-postgres/).

To produce a stricly ordered Stream, Sequin uses a combination of the source table and a replication slot to produce a Stream. This makes it possible to [consume rows](#consuming-rows) from a table. When rows are inserted or updated, Sequin adds the rows to the end of the Stream so consumers will process them. Sequin stores Stream information in its internal catalog tables.

When you setup a Stream for a table in Sequin, you must specify its **sort column**. Sequin will use this sort column to produce the initial Stream. You can also use the sort column when rewinding a consumer, as it approximates the Stream's order.

<Note>
The sort column orders historical rows in the Stream. For new and updated rows, their order in the Stream will depend on when they are committed. This is because auto-increment and datetime columns will [not necessarily commit in order in Postgres](https://blog.sequinstream.com/postgres-streams-can-commit-out-of-order).

After the Stream starts, the order of rows in the Stream will be the order they appear in Sequin's replication slot. This corresponds to the latest entry of the row in the WAL (i.e. the most recent [LSN](https://www.postgresql.org/docs/current/datatype-pg-lsn.html) corresponding to a given row).
</Note>

It's important you choose the right sort column. The right sort column is either:

- A **timestamp** like `updated_at` that your system updates whenever a row is inserted or updated.
- A **Postgres stream** like `idx` or `index` that your system increments whenever a row is inserted or updated.

If your table does not have a timestamp or integer column that is updated on insert/update, you can add one:

<AccordionGroup>
  <Accordion icon="clock" title="Adding an updated_at column">
    Here's how you can add an `updated_at` column to your table and create a trigger that updates it on changes:

    ```sql
    -- Add the updated_at column
    alter table your_table add column updated_at timestamp default now();

    -- Update existing rows (if needed)
    -- This will add *some* sort to existing rows, which may be desirable.
    -- (Note if your table has millions of rows, this could take a while)
    update your_table set updated_at = inserted_at;

    -- Create a function to update the timestamp
    create or replace function update_timestamp()
    returns trigger as $$
    begin
      new.updated_at = now();
      return new;
    end;
    $$ language plpgsql;

    -- Create a trigger to call the function
    create trigger set_timestamp
    before update on your_table
    for each row
    execute function update_timestamp();
    ```

    This will ensure that the `updated_at` column is set to the current time on insert and updated whenever a row is modified.
  </Accordion>

  <Accordion icon="list-ol" title="Adding a sequence column">
    Here's how you can add an auto-incrementing `idx` column to your table:

    ```sql
    -- Create a sequence
    create sequence your_table_idx_seq;

    -- Add the idx column
    alter table your_table add column idx integer default nextval('your_table_idx_seq');

    -- Update existing rows (if needed)
    -- (Note if your table has millions of rows, this could take a while)
    update your_table set idx = nextval('your_table_idx_seq');
    ```

    This will ensure that the `idx` column is automatically incremented for new rows. For updates, you'll add this trigger:

    ```sql
    create or replace function update_idx()
    returns trigger as $$
    begin
      new.idx = nextval('your_table_idx_seq');
      return new;
    end;
    $$ language plpgsql;

    create trigger set_idx
    before update on your_table
    for each row
    execute function update_idx();
    ```

    This trigger will ensure the `idx` is updated on row updates.
  </Accordion>
</AccordionGroup>

<Note>If your table is append-only, you can use `inserted_at` or an auto-incrementing `id` column as the sort column. `uuid` columns will not work as they are not sequential.</Note>

## Rows

Sequin converts Postgres rows into JSON objects that are then delivered to your consumers.

A row has this shape:

```js
{
  record: {
    [key: string]: any;
  };
  metadata: {
    table_schema: string;
    table_name: string;
    consumer: {
      id: string;
      name: string;
    };
  };
}
```

For example:

```json
{
  "record": {
    "id": 1,
    "name": "Paul Atreides",
    "title": "Duke of Arrakis",
    "spice_allocation": 1000,
    "is_kwisatz_haderach": true
  },
  "metadata": {
    "table_schema": "public",
    "table_name": "house_atreides_members",
    "consumer": {
      "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
      "name": "dune_characters_consumer"
    }
  }
}
```

## Consuming rows

After creating a Stream, you have a few options for consuming rows with Sequin:

### Consumer Groups

With [**Consumer Groups**](/consume/consume-api/overview), you can setup an HTTP endpoint that your apps and services pull rows from. The endpoint follows the popular consumer group pattern of other queues and streams:

1. You can have many **consumers** (processes or workers) all pulling from the consume endpoint at the same time.
2. After a batch of rows is delivered to a consumer in the group, that consumer has a certain period of time to process the rows. This **visibility timeout** is configurable at the group level.
3. After processing a batch of rows, the consumer acknowledges them by calling the [acknowledge endpoint](/consume/consume-api/ack).
4. If the consumer fails to process the batch, rows in the batch will be made available again for delivery to other consumers in the group.
5. When a row is updated in the database, it is re-delivered to the group.

You can setup many consumer groups for a single table.

[Read more](/consume/consume-api/overview) about the Consumer Group API endpoints.

<Frame>
  <img src="/images/core/pull-instructions.png" alt="The Sequin UI, displaying instructions for pulling messages from a consumer. The instructions are curl requests to the consumer's endpoint." style={{ maxWidth: '500px' }} />
</Frame>

### Webhooks

Sequin's [Webhook Subscriptions](/consume/webhooks) are powered by [Consumer Groups](#consumer-groups). Each webhook payload contains one row. Rows are processed exactly once per subscription.

If your webhook endpoint returns a non-200 status code or fails to process the webhook within a configurable timeout, the row will be re-delivered.

Sequin will backoff and retry webhook delivery indefinitely.

<Frame>
  <img src="/images/core/http-endpoint-config.png" alt="The Sequin UI, prompting you to specify the HTTP endpoint to send messages to." style={{ maxWidth: '500px' }} />
</Frame>

You can monitor webhook status on the Sequin dashboard, including failing and outstanding messages.

{/* todo: image here */}

### Sync API

<Note>
The sync API is still in development.
</Note>

The **Sync API** is a simple alternative to [Consumer Groups](#consumer-groups). With the Sync API, Sequin presents your table as a paginateable endpoint. You can start paginating from any point in the table. When you reach the end, you can long-poll to receive new and updated rows in real-time.

The Sync API is great for situations where you want to manage the state of the cursor and don't need multiple consumers to process a stream in parallel.

## Filtering

In all of Sequin's consuming paradigms, you can specify one or more **filters** to process a subset of a table's rows or changes. The filters support SQL operators:

<Frame>
  <img src="/images/core/filters-config.png" alt="The Sequin UI, prompting you to specify the filters for a consumer. There is a list of columns to choose from, a list of operators, and an input field for values." style={{ maxWidth: '500px' }} />
</Frame>

## Grouping

In all of Sequin's consuming paradigms, you can specify one or more **grouping columns** to group messages during delivery.

Grouping ensures that messages with the same values in the specified grouping columns are processed in a strict order relative to each other. This means that for a given group, messages are always processed one by one, in the order they were received.

However, messages belonging to different groups may be processed concurrently or out of order relative to each other.

Grouping is particularly useful when you need to maintain consistency for related messages while still allowing parallel processing across different groups.

### Transforming

<Note>
Transforms are still under development and coming soon.
</Note>

You can **transform** messages before they're delivered to your consumers using a Lua function. This is useful for transforming data into a format more suitable for your application.

## Change Capture Pipelines

Sequin streams rows directly from your tables. But sometimes, you want to log every discrete change to rows, and capture the `old` and `new` values of that change. Or, you want to retain a log of deleted rows.

With **Change Capture Pipelines**, Sequin captures changes from tables you specify. Sequin will write each change as it happens to a table in your database. Then, you can setup a Stream for your event table to stream its rows.

Change Capture Pipelines can indicate which operations to capture (i.e. `insert`, `update`, `delete`) as well as apply SQL filtering on changes.

If you don't want to keep every change indefinitely, you can setup your target event table with a retention policy using extensions such as `pg_cron` or `pg_partman`. By retaining recent changes in a Postgres table, you get to use Sequin features like replay and rewind.

Event tables look like this:

```sql
create table your_event_log (
  id serial primary key,
  seq bigint not null,
  source_database_id uuid not null,
  source_table_oid bigint not null,
  source_table_schema text not null,
  source_table_name text not null,
  record_pk text not null,
  record jsonb not null,
  changes jsonb,
  action text not null,
  committed_at timestamp with time zone not null,
  inserted_at timestamp with time zone not null default now()
);
```

[Learn more about Change Capture Pipelines](/capture-changes/change-capture-pipelines).

## Replication slots

To maintain Streams, Sequin needs to detect changes in your database. Core to that process is a [replication slot](https://www.postgresql.org/docs/9.4/catalog-pg-replication-slots.html) and a corresponding [publication](https://www.postgresql.org/docs/current/logical-replication-publication.html). The publication determines which tables are captured by the replication slot.

Replication slots ensure Sequin never misses a change, even if Sequin is disconnected from your database for a period of time.

## Guarantees

### Strict ordering

Sequin guarantees that messages with the same primary key are processed in order for a given consumer.

Sync endpoints are strictly ordered. For the Consumer Group API and webhooks, Sequin will not deliver messages to a consumer until the previous messages for that primary key has been acknowledged.

This is useful for applications that need to process changes to a single record in order. For example, if you're processing changes to a user's profile, you want to make sure that the changes are applied in order.

### Exactly-one processing

Both the Consume Groups and Webhook Subscriptions have exactly-once processing guarantees.