---
title: "How to create audit logs with Sequin"
sidebarTitle: "Create audit logs"
description: "Create a reliable log of all changes in your database"
---

Audit logs are an essential tool for tracking and recording changes in your database. With Sequin, you can create comprehensive audit logs of changes in your Postgres database that help you:

- **Maintain compliance**: Meet regulatory requirements by tracking all data modifications
- **Debug issues**: Trace the history of changes to investigate problems
- **Build customer features**: Create activity feeds, change history views, and even undo/redo functionality for your users
- **Data recovery**: Maintain a backup of deleted or modified data, enabling recovery from accidental deletions or unwanted changes

With Sequin, you can turn Postgres' Write Ahead Log (WAL) into a stream. Your application can pull rows from that stream using a Consumer Group or receive rows via a Webhook Subscription. Then, your application can upsert rows into a destination table.

Sequin ensures ordered delivery and exactly-once processing, so you don't need to worry about race conditions or missing rows.

### Comparison with other solutions

Here's how Sequin's audit logging approach compares to alternatives:

- **Triggers**: Traditional Postgres triggers require writing PL/pgSQL and can impact database performance. Sequin moves the processing logic to your application code where it's easier to maintain.

- **`listen`/`notify`**: While lightweight, Postgres' built-in pub/sub is unreliable - messages can be missed if consumers are down. Sequin guarantees exactly-once processing.

- **Debezium/Kafka**: These solutions require running and maintaining additional infrastructure. Sequin keeps your data in Postgres and runs as a single Docker container.

- **Manual application logging**: Implementing audit logging in your application code is error-prone and can miss database changes that happen outside your app. Sequin captures all changes directly from Postgres' WAL.

## Prerequisites

This guide assumes you already have Sequin installed and have a database connected. If you don't, see the [quickstart guide](/quickstart).

## Overview

This guide shows you how to create an audit log of discrete changes in your database. For example, if you have a `subscriptions` table and you want to capture every single update that happens to `subscriptions`, along with the old and new values of fields like `status`, `value`, `start_date`, etc.

As you'll see, the destination table can be in the same database as the source table, or a different database.

Sequin will capture changes to your tables in a `sequin_events` table. The `sequin_events` table might be sufficient for your audit log needs. If that's the case, you'll perform the following steps:

1. **[Create a Change Capture Pipeline](#create-a-change-capture-pipeline)**: The Pipeline will capture changes to tables in your database to a `sequin_events` table.
2. (Optional) **[Create a View](#create-a-view)**: Use a Postgres view to shape the schema into a format convenient for your use case.

However, if you need to transform the data before writing it to an audit table you control, you'll perform the following steps:

1. **[Create a Change Capture Pipeline](#create-a-change-capture-pipeline)**: The Pipeline will capture changes to tables in your database to a `sequin_events` table.
2. **[Setup the audit table](#setup-the-audit-table)**: Create the audit table you want to write the changes to.
3. **[Create a stream](#create-a-stream)**: The stream will contain events from the `sequin_events` table.
4. Either **[Create a Consumer Group](#create-a-consumer-group)** or **[Create a Webhook Subscription](#consume-events-with-webhooks)**: Use either tool to process the changes from the stream.
5. **[Upsert events to the audit table](#upsert-events-to-the-audit-table)**: Write the upsert logic in your worker code.

### Start locally

You can start by following all the steps below in your local environment. At the end of this guide are instructions for [deploying to production](#deploying-to-production).

## Create a Change Capture Pipeline

You can use Sequin's [Change Capture Pipeline](/capture-changes) to stream changes from your table(s) to an event table in your database.

You'll create a Change Capture Pipeline for each table you want to create an audit log for. You can have multiple pipelines write to the same `sequin_events` table. Then, later, you can stream `sequin_events` to your application in order to ingest the changes into your audit table:

  <Frame>
    <img src="/images/how-tos/audit-logging/end-to-end-diagram.png" alt="Overview of the Change Capture Pipeline" />
  </Frame>

<Steps>
  <Step title="Create a Change Capture Pipeline">

    In the Sequin web console, click on the "Change Capture Pipelines" tab. Click "Create Pipeline".

    Under "Source configuration", select the first table you want to create an audit log for.

    You can specify which operations you want to capture. For an audit log, you'll most likely want to capture every operation (e.g. `insert`, `update`, `delete`):

    <Frame>
      <img style={{maxWidth: '500px'}} src="/images/how-tos/audit-logging/source-config.png" alt="Source configuration" />
    </Frame>

    You can also specify filters to narrow down the changes you want to capture. For example, maybe you only want to capture changes for `subscriptions` that currently have an `mrr` greater than $100.
  </Step>
  <Step title="Create an event table">

    For the destination, assuming you don't have a `sequin_events` table yet, click "Create new event table". That will open a modal with instructions for creating the table:

    <Frame>
      <img style={{maxWidth: '500px'}} src="/images/how-tos/audit-logging/create-event-table-modal.png" alt="Create event table modal" />
    </Frame>

    One important decision you need to make is what **retention policy** to use for the `sequin_events` table. The retention policy specifies how long Postgres should keep event data in the `sequin_events` table.

    If you're going to use the `sequin_events` table as your audit log, your retention policy will be dictated by your product's requirements. For example, if you're showing a user a history of changes in their account, do you want to show them just the changes from the last 30 days? Or all the changes ever?

    If you're going to use the `sequin_events` table as an intermediate table that you'll stream to your system before writing it to your audit table, you can use a shorter retention policy. In this instance, the events table is a _buffer_. It's advisable to give yourself a lenient buffer, such as 14 days. This means your system/pipeline can be down for a few days without losing data. Or, if you discover that you recently deployed a bug to your processing pipeline, you can rewind and re-process data from the last 14 days.

    We recommend using `pg_cron` to manage your retention policy starting out, unless you anticipate you'll be starting with >10M writes to the `sequin_events` table per day. (Even then, `pg_cron` can handle it on a decent-sized machine!)

  </Step>
  <Step title="Create Change Capture Pipelines for other tables">

    After creating your first Change Capture Pipeline, you can create additional pipelines for other tables.

    You can have all tables write to the same `sequin_events` table. That's usually the simplest way to start. Even if later you plan on writing each table to its own destination audit table, you can easily filter the `sequin_events` table by source table as needed.
  </Step>
  <Step title="Confirm your Change Capture Pipelines are working">

    To confirm your pipelines are working, you can make some changes to your source tables. When you query the `sequin_events` table, you should see the changes reflected there.

    <Check>
      You've setup Change Capture Pipelines for all your tables. Changes to your tables should now be captured in the `sequin_events` table.
    </Check>
  </Step>
</Steps>

## (Optional) Create a view

It's possible the schema of Sequin's `sequin_events` table is sufficient to serve as your audit log. If that's the case, you can stop here! There's no need to stream the `sequin_events` table to your workers to ingest the changes into another audit table.

Or, the `sequin_events` table might be _close_, but you just want to make a few tweaks. If that's the case, you can create a Postgres view to shape the schema into a format convenient for your use case.

For example, maybe you want to make a view called `subscription_logs` that contains only the logs for the `subscriptions` table. And maybe you want to pull a few fields out of the `record` JSONB column in `sequin_events` and into a top-level column in the view, like `subscription_id` and `status`:

```sql
create view subscription_logs as
select
    id,
    seq,
    action,
    record_pk,
    record->>'id' as subscription_id,
    record->>'status' as status,
    record,
    changes,
    created_at
from sequin_events
-- or, you can use the `source_table_oid` column instead, which is
-- silghtly more robust
where table_name = 'subscriptions'
order by seq desc;
```

`subscription_logs` might be a bit more ergonomic to query with your ORM or internal tooling.

### Indexes

If you'll be making a lot of queries against your audit log/view, you might consider adding indexes to improve performance.

For example, if you'll be querying by `record->>'subscription_id'`, you might consider adding an index on that column:

```sql
create index on sequin_events ((record->>'status')) 
where table_name = 'subscriptions';
```

Notably, you'll want to create **partial indexes** like this (e.g. `where table_name = 'subscriptions'`). That's because non-subscription entries in the `sequin_events` table will have a different shape. If you won't be querying them by `status`, you don't need to include them in that index.

### Generated columns

You might be tempted to add generated columns to your `sequin_events` table to hoist fields out of the `record` JSONB column (for example, hoisting up `record->>'status'` to a top-level column).

We generally recommend against using generated columns, as they're harder to manage. To add a generated column to a table, Postgres needs to exclusively lock the table while the column is being added. This can take a while on large tables.

Instead, see how far you can get with views and indexes. If you start to hit limits there, you may have reached a point where you should stream the `sequin_events` table to your application to ingest the changes into another audit table with a schema you fully control.

<Check>
   If the `sequin_events` table is sufficient for your audit log needs, you can stop here. Otherwise, continue on to [Setup your audit table](#setup-your-audit-table) so you can consume the changes into your audit table.
</Check>

## Setup your audit table

If you want to ingest the changes from the `sequin_events` table into an audit table you own, create that audit table.

The most important consideration is that you'll want your [upsert logic](#upserting-into-your-audit-table) to be idempotent. That means you'll use an `insert into ... on conflict` statement.

We recommend adding a column to your audit table, such as `event_id` or `sequin_event_id` which corresponds to the `id` column in the `sequin_events` table.

Then, you can add a unique constraint on this column:

```sql
create unique index my_audit_logs_event_id_idx on my_audit_logs(event_id);
```

This ensures that events can only be written to your audit table once.

## Create a stream

In order to consume the rows in the `sequin_events` table, you can create a stream. Creating a stream of a table makes it available to consumer groups or destinations like webhooks:

Navigate to the "Streams" tab in the Sequin web console and click "Create Stream".

Select the `sequin_events` table as the source and click "Create Stream".

## Consume events with a Consumer Group

Sequin is capturing changes to your tables into the `sequin_events` table, which is connected to a Stream. You can setup a worker to process these changes, transforming them as needed before [upserting them into your audit table](#upserting-into-your-audit-table).

This section will show you how to consume the rows in the `sequin_events` table with a Consumer Group via the [Consumer Group API](/consume/consume-api/overview).

<Info>
  If you'd prefer to receive webhooks, skip down to the [Consume events with webhooks](#consume-events-with-webhooks) section.
</Info>

### How many Consumer Groups do you need?

A Consumer Group lets you consume from a Sequin Stream in parallel, safely. You can have 1 or 100 workers all pulling from the same Stream. Rows from your `sequin_events` table will be processed by the group only once, distributing the load across your workers.

Should you have one Consumer Group process the entire `sequin_events` table? Or split it up, perhaps creating one Consumer Group for each source table?

The right design depends on two factors:

- How high is your event volume?
- Will you have one destination audit table, or multiple?

If your event volume is low, it may be sufficient to have a single Consumer Group process the entire `sequin_events` table – even if you're writing to multiple destination audit tables. Workers can pull a batch of events, group them by destination table, and fan out upserts to each destination table.

Otherwise, you may consider having a separate Consumer Group for each destination audit table. Workers will pull a batch of events, write them to the appropriate destination table, and then grab the next batch.

**Advantages of one Consumer Group:**

- less setup
- potentially fewer worker clusters to manage

**Advantages of multiple Consumer Groups:**

- higher throughput
- pipelines for each table can be scaled independently
- pipelines for each table have their own health
- the worker code is simpler, since there is no grouping and fan-out logic

### Create a Consumer Group

<Steps>
  <Step title="Create a Consumer Group">
    Click on the "Consumer Groups" tab in the Sequin web console. Click "Create Consumer Group".
  </Step>

  <Step title="Configure the source">
    Under "Source", select the `sequin_events` Stream.

    Under "Records to process", optionally add a filter. For example, if you want this Consumer Group to only process events for the `subscriptions` table, you can add a filter on `table_name = 'subscriptions'`:

    <Frame>
      <img style={{maxWidth: '500px'}} src="/images/how-tos/audit-logging/records-to-process.png" alt="List of available filters to apply to the changes you want to capture" />
    </Frame>

    For "Where should the Consumer Group start?", you can leave the default option selected, "At the beginning of the table." This means your Consumer Group will process anything already in the `sequin_events` table, as well as any new changes.
  </Step>

  <Step title="Configure message grouping">
    Under "Message grouping", leave the default option selected to ensure events for the same row are processed in order.
  </Step>

  <Step title="Configure Consumer Group settings">
    Under "Consumer Group configuration", you can tweak settings like the "visibility timeout" as desired.
    
    The visibility timeout specifies how long workers/consumers in your consumer group should be given to process their batch of messages. If they don't finish processing within the visibility timeout, the messages will be made available to other consumers in the group.

    Pick a value that makes sense given how long your upserts to your audit table will take. The default is 30 seconds, which should be more than reasonable for most use cases.

    <Info>
      As you'll see, inserting into your audit table should be an idempotent operation. So it's OK if messages are processed more than once.
    </Info>
  </Step>

  <Step title="Create and test the Consumer Group">
    Click "Create Consumer Group".

    To make sure things are working end-to-end, you can copy the example `curl` request on the Consumer Group's page and run it. The endpoint should return a batch of messages from your `sequin_events` table.
  </Step>
</Steps>

<Check>
  You're ready to consume from the `sequin_events` Stream in your application.
</Check>

### Setup your application

In your application, you'll setup a worker that pulls messages from the Consumer Group and upserts them into your audit table.

Here's a basic example using Sequin's Go SDK to consume events:

```go
// Example worker that processes events from a Sequin consumer group
// and upserts them into an audit log table
func main() {
    client := sequin.NewClient(&sequin.ClientOptions{
        Token: os.Getenv("SEQUIN_TOKEN"),
    })

    // Create a new processor for the consumer group
    processor, err := sequin.NewProcessor(
        client,
        "my-consumer-group",
        processEvents,
        sequin.ProcessorOptions{
            MaxBatchSize: 100, // Process up to 100 events at once
            MaxConcurrent: 10,
        },
    )
    if err != nil {
        log.Fatalf("Failed to create processor: %v", err)
    }

    // Start processing events
    if err := processor.Start(); err != nil {
        log.Fatalf("Processor failed: %v", err)
    }
}

// processEvents handles a batch of events from the consumer group
// for simplicity, it doesn't batch the upserts into one statement
func processEvents(ctx context.Context, msgs []sequin.Message) error {
    // See below for the logic to upsert events into your audit table
    res, err := upsert_events(ctx, msgs)
    if err != nil {
        return err
    }

    log.Printf("Successfully processed %d events", len(msgs))
    return nil
}
```

For `MaxBatchSize`, if you think it will difficult for you to [batch upsert rows](#upserting-into-your-audit-table) with your ORM or database adapter, set it to `1`. Otherwise, we recommend you set it to a batch size of at least 100. This will improve overall throughput. (Depending on your hardware and use case, you'll see diminishing returns upserting batch sizes greater than 1000 into Postgres.)

See "[Upserting into the audit table](#upserting-into-the-audit-table)" for the implementation details of `upsert_events`.

<Info>
  See the [Go SDK](https://github.com/sequinstream/sequin-go/tree/main/examples/audit_logging) for a complete example of consuming events from a Consumer Group with Go.
</Info>

### Repeat for other tables

If you're creating consumer groups for multiple tables, repeat the process for each table.

## Consume events with webhooks

Instead of pulling events from a consumer group, you can have Sequin push events to a webhook endpoint. Each webhook payload will contain a batch of events. Events are processed exactly once per subscription.

### Handle webhooks in your application

Your webhook endpoint will receive POST requests with [event payloads](/consume/webhooks).

Here's an example of handling webhooks with Node.js and Express:

```javascript
app.post('/webhooks/audit-events', async (req, res) => {
  try {
    // The webhook payload contains an array of records in req.body.data
    const events = req.body.data.map(item => ({
      eventId: item.record.id,
      action: item.record.action,
      oldValues: item.record.changes,
      newValues: item.record,
      tableName: item.metadata.table_name,
      tableSchema: item.metadata.table_schema
    }));
    
    // Batch upsert all events into your audit table
    await upsertEvents(events);

    // Return 200 to acknowledge successful processing
    res.sendStatus(200);
  } catch (error) {
    console.error('Error processing webhook:', error);
    // Return 500 to trigger a retry
    res.sendStatus(500);
  }
});
```

If your endpoint returns a non-200 status code or fails to respond within the configured timeout, Sequin will retry the webhook indefintely with an exponential backoff.

### Create a webhook subscription

With your webhook endpoint ready, you can create a subscription:

<Steps>
  <Step title="Navigate to Webhook Subscriptions">
    In the Sequin web console, under destinations, navigate to the Webhook Subscriptions tab and click "Create Webhook Subscription".
  </Step>

  <Step title="Configure the source">
    Under Source, select the `sequin_events` Stream.

    Under "Records to process", optionally add a filter. For example, if you want this Webhook Subscription to only receive events for the `subscriptions` table, you can add a filter on `table_name = 'subscriptions'`.
  </Step>

  <Step title="Configure the webhook">
    Under "Webhook configuration", enter your endpoint URL. This is where Sequin will send the webhook payloads.

    For "Batch size", if you think it will difficult for you to [batch upsert rows](#upserting-into-your-audit-table) with your ORM or database adapter, leave at the default value of `1`. Otherwise, we recommend you set it to a batch size of at least 100. This will improve overall throughput. (Depending on your hardware and use case, you'll see diminishing returns upserting batch sizes greater than 1000 into Postgres.)

    Remember, you'll want to insert all rows you receive in a webhook payload in a single statement.
  </Step>

  <Step title="Configure message grouping">
    Under "Message grouping", leave the default option selected to ensure events for the same row are processed in order.
  </Step>

  <Step title="Create the subscription">
    Click "Create Webhook Subscription".
  </Step>
</Steps>

## Upserting into your audit table

You'll upsert events into your audit table using an `insert into ... on conflict` statement. This will ensure that events can only be written to your audit table once.

The conflict key will be the `event_id` column you [added to your audit table](#setup-your-audit-table).

The process will be:

1. Map events to your audit table
2. Perform the upsert, ideally in a single statement

### Map events to your audit table

You'll flatten events from a nested JSON structure into a flat structure that matches your audit table.

Sequin sends events to your consumer in JSON. JSON's types are not as rich as Postgres' types. So you may need to cast the types appropriately.

Here's a list of JSON types that you might need to cast before upserting into Postgres:

- **Timestamps/dates**: Transported in JSON as strings, need casting to `timestamp` or `date`
- **UUIDs**: Transported in JSON as strings, need casting to `uuid`
- **Numeric types**: Transported in JSON as numbers, might need explicit casting to `decimal`, `bigint`, etc. depending on precision requirements

### Batch upserts

Ideally, you batch your upserts. For example, if you're processing a batch of 100 events, you'll upsert all 100 events in a single statement. This is far more efficient than upserting 100 individual events.

If you're using an ORM, consult your ORM's documentation for how to perform batched upserts. For example, here's what upserts look like with `ActiveRecord`:

```ruby
class Event < ApplicationRecord
  def self.batch_upsert_events(events_data)
    # Convert array of hashes to array of Event objects if needed
    events = events_data.map { |data| new(data) }
    
    # Perform the upsert
    Event.upsert_all(
      events.map(&:attributes),
      unique_by: :event_id,
      # Update all columns except id and created_at
      update_only: Event.column_names - ['id', 'created_at'],
      # Optional: returns the event_ids that were affected
      returning: :event_id
    )
  end
end
```

In raw SQL, that looks something like:

```sql
insert into events (
  event_id,
  name,
  status,
  metadata,
  updated_at
) values 
  ($1, $2, $3, $4, $5),
  ($6, $7, $8, $9, $10),
  ($11, $12, $13, $14, $15)
on conflict (event_id) do update set
  name = excluded.name,
  status = excluded.status,
  metadata = excluded.metadata,
  updated_at = excluded.updated_at;
```

Importantly, we recommend using a `do update` clause as opposed to a `do nothing` clause. This means that if the event already exists in your audit table, you'll update it with new values. This can be handy if you deployed a bug to your worker code, and are now replaying events in order to "patch over" old, incorrect event values.

## Deploying to production

Assuming you've followed the steps so far in your local environment, the next step is to deploy your implementation to production.

You can use the Sequin CLI to export your local configuration and apply it to your production environment.

<Steps>
  <Step title="Perform migrations in your production database">
    You'll need to ensure your production database has a `sequin_events` table to write to. If you're using a migration system, add that table to your migrations.

    Likewise, you'll need to ensure your production database has any audit tables that you'll want your system to write to.
    
    We recommend deploying and migrating before proceeding.
  </Step>

  <Step title="Ensure your Sequin token is set in production">
    If you're using a Consumer Group to pull messages from Sequin, that request uses a Sequin token.

    Ensure the token is configurable, e.g. controlled by an environment variable.

    Then, ensure your production environment has the token set.
  </Step>

  <Step title="Export your local configuration">
    You can use Sequin's YAML configuration file to export your local configuration and import it into your production environment.

    After [setting up the Sequin CLI](/cli), run the following command to export your local configuration:

    ```bash
    sequin config export --context=local
    ```

    This will create a `sequin.yaml` file in your current working directory. You can commit this file to your repository to share the configuration with your team.

    <Info>
      You might consider having different `sequin.yaml` files for different environments (local, staging, production). For example, you might have `sequin.staging.yaml` and `sequin.production.yaml`.

      That's because you may name things differently in your local environment vs. production. For example, your database may be called `my_db_dev` in local but `my_db_prod` in production.
    </Info>

    <Warning>
      The `sequin config export` command is experimental. You'll need to manually tweak the exported file before using it. See the warning in the CLI output.
    </Warning>
  </Step>

  <Step title="Apply the configuration to your production environment">
    To apply the configuration to your production environment, in the same directory as your `sequin.yaml` file, run the following command:

    ```bash
    sequin config apply sequin.yaml --context=production
    ```
  </Step>
</Steps>

