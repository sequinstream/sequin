---
title: "How to maintain caches with Sequin"
sidebarTitle: "Maintain caches"
description: "Keep your caches warm and up-to-date using Sequin"
---

Sequin makes it easy to maintain caches that stay in sync with your database. Common use cases include:

- **Redis caching**: Keep frequently accessed data cached in Redis for fast retrieval
- **In-memory caches**: Maintain application-level caches like Memcached or application memory
- **Search indexes**: Keep search indexes like Elasticsearch up-to-date with your database

With Sequin, you can turn tables in your Postgres database into streams. Your application can pull rows from a stream using a Consumer Group or receive rows via a Webhook Subscription. Then, your application can update your cache with the latest data.

Sequin ensures ordered delivery and exactly-once processing, so you don't need to worry about race conditions or stale cache entries. This is especially important when maintaining caches that need to reflect the current state of your database.

This guide shows you how to:

- **Warm your cache**: Populate your cache with initial data
- **Keep it fresh**: Maintain an up-to-date cache as source data changes
- **Rebuild when needed**: Re-warm your cache if it gets flushed

## Prerequisites

This guide assumes you have:

- Sequin installed and databases connected (if not, see the [quickstart guide](/quickstart))
- A caching system ready (Redis, Memcached, etc.)
- An application that can pull rows from a Consumer Group or receive rows via a Webhook Subscription

## Overview

To maintain a cache with Sequin, you'll:

1. **[Create a stream](#create-a-stream)**: Stream changes from your source data
2. Either **[Create a Consumer Group](#create-a-consumer-group)** or **[Create a Webhook Subscription](#consume-with-webhooks)**: Choose how to receive updates
3. **[Write cache operations](#writing-cache-operations)**: Implement cache update logic

### Cache invalidation

Sequin streams the latest version of rows as they're created and updated in a table. Because Sequin streams rows from tables, and deleted rows are removed from tables, Sequin can't "see" deleted rows.

When data is deleted from your source, you might want to remove it from your cache. You have several options:

1. **Soft delete tracking**: Use something like a `status` or `deleted_at` column in your source table. Then, when processing rows from your stream, you can remove the corresponding cache entry.
2. **Change Capture Pipeline**: Use [Change Capture Pipelines](/capture-changes) to track explicit delete operations. Process these events to remove entries from your cache.

### Development workflow

Start by testing your cache maintenance pipeline locally. Later, we'll cover [production deployment](#deploying-to-production).

## Create a stream

First, create a stream for each one of your source tables. This stream will let you backfill your cache with historical data, and provide real-time updates to keep your cache fresh:

<Steps>
  <Step title="Navigate to Streams">
    In the Sequin web console, click on the "Streams" tab. Click "Create Stream".
  </Step>

  <Step title="Select your source table">
    Select the table containing the data you want to cache.

    Under "Sort and start", choose a sort column that reflects when data changes. `updated_at` works well for most cases since it captures both new and modified data.

    <Info>
      The sort column determines the order of updates to your cache when processing historical data. [Read more about sort columns](/how-sequin-works#streams).
    </Info>

    <Frame>
      <img style={{maxWidth: '500px'}} src="/images/how-tos/replicating-tables/create-stream.png" alt="Stream configuration" />
    </Frame>
  </Step>

  <Step title="Create the stream">
    Click "Create Stream".
  </Step>
</Steps>

## Create a Consumer Group

You can use a Consumer Group to pull rows from your stream and write them to your cache. You'll need one Consumer Group per stream:

<Steps>
  <Step title="Create a Consumer Group">
    Click on the "Consumer Groups" tab in the Sequin web console. Click "Create Consumer Group".
  </Step>

  <Step title="Configure the source">
    Under "Source", select the stream you created.

    For "Records to process", you can optionally add filters to sync a subset of your source table to the destination. For example, you can filter for `subscriptions` with a certain `status` or orders over a certain `total_amount`:

    <Frame>
      <img style={{maxWidth: '500px'}} src="/images/how-tos/replicating-tables/records-to-process.png" alt="Records to process" />
    </Frame>

    You can always change these filters later and [re-backfill your cache](#re-backfill-the-destination).

    For "Where should the consumer group start?", you can indicate how far back in time you want to sync rows from the source table. Rows are ordered by the sort column you chose when you created the stream. So if you chose a field like `updated_at`, you can choose to sync rows from the beginning of the table, from the last few months, etc.

    If you want your cache to contain all rows from the source, choose "At the beginning of the table." If you want to sync only new rows, choose "At the end of the table." Otherwise, choose a specific point in time according to your needs.
  </Step>

  <Step title="Configure message grouping">
    Under "Message grouping", leave the default option selected.

    <Info>
      Sequin will group messages by primary key. This means that if a consumer pull or webhook push is outstanding for a row, Sequin will not deliver that row until the outstanding message is processed. This ensures there's no risk of a race condition as you write rows to your cache.
    </Info>
  </Step>

  <Step title="Create and test the Consumer Group">
    Click "Create Consumer Group".

    To make sure things are working end-to-end, you can copy the example `curl` request on the Consumer Group's page and run it. The endpoint should return a batch of rows from your source table.
  </Step>
</Steps>

### Setup your application

In your application, you'll setup a worker that pulls messages from the Consumer Group and upserts them into your cache.

For `MaxBatchSize`, because cache update operations are usually idempotent, it's usually safe to process batches ([see more below](#updating-your-cache)). A `MaxBatchSize` of 100 is a good place to start.

Here's a basic example using Sequin's Go SDK:

```go
func main() {
    client := sequin.NewClient(&sequin.ClientOptions{
        Token: os.Getenv("SEQUIN_TOKEN"),
    })

    processor, err := sequin.NewProcessor(
        client,
        "my-consumer-group",
        processRows,
        sequin.ProcessorOptions{
            MaxBatchSize: 100,  // Process up to 100 rows at once
            MaxConcurrent: 10,
        },
    )
    if err != nil {
        log.Fatalf("Failed to create processor: %v", err)
    }

    if err := processor.Start(); err != nil {
        log.Fatalf("Processor failed: %v", err)
    }
}

func processRows(ctx context.Context, msgs []sequin.Message) error {
    // See below for the logic to update your cache
    res, err := updateCache(ctx, msgs)
    if err != nil {
        return err
    }

    log.Printf("Successfully processed %d rows", len(msgs))
    return nil
}
```

<Info>
  See the [Go SDK](https://github.com/sequinstream/sequin-go/tree/main/examples/audit_logging) for a complete example of consuming events from a Consumer Group with Go.
</Info>

### Rewind as needed

Remember, when you're in development, you can rewind your Consumer Group to any point in time. As you're tweaking your pipeline and cache update logic, you can rewind your Consumer Group in the Sequin web console.

## Consume with webhooks

Instead of pulling rows with a Consumer Group, you can have Sequin push them to a webhook endpoint:

### Handle webhooks in your application

Your webhook endpoint will receive POST requests with a [batch of one or more rows](/consume/webhooks).

Here's an example of handling webhooks with Node.js and Express:

```javascript
app.post('/webhooks/cache', async (req, res) => {
  try {
    // Webhook payload contains a data array of rows
    const rows = req.body.data;
    
    await updateCache(rows);

    // Return 200 to acknowledge successful processing
    res.sendStatus(200);
  } catch (error) {
    console.error('Error processing webhook:', error);
    // Return 500 to trigger a retry
    res.sendStatus(500);
  }
});
```

If your endpoint returns a non-200 status code or fails to respond within the configured timeout, Sequin will retry the webhook indefinitely with an exponential backoff.

### Setup your subscription

With your webhook endpoint ready, you can create a subscription:

<Steps>
  <Step title="Navigate to Webhook Subscriptions">
    In the Sequin web console, under destinations, navigate to the Webhook Subscriptions tab and click "Create Webhook Subscription".
  </Step>

  <Step title="Configure the source">
    Under Source, select your stream.
  </Step>

  <Step title="Configure the webhook">
    Under "Webhook configuration", enter your endpoint URL. This is where Sequin will send the webhook payloads.

    For "Batch size", because cache update operations are usually idempotent, it's usually safe to process batches ([see more below](#updating-your-cache)). A batch size of 100 is a good place to start.

    Remember, you'll want to insert all rows you receive in a webhook payload in a single statement.
  </Step>

  <Step title="Configure message grouping">
    Under "Message grouping", leave the default option selected to ensure rows are processed in order.

    <Info>
      Sequin will group messages by primary key. This means that if a consumer pull or webhook push is outstanding for a row, Sequin will not deliver that row until the outstanding message is processed. This ensures there's no risk of a race condition as you write rows to your cache.
    </Info>
  </Step>

  <Step title="Create the subscription">
    Click "Create Webhook Subscription".
  </Step>
</Steps>

### Rewind as needed

Remember, when you're in development, you can rewind your Webhook Subscription to any point in time. As you're tweaking your pipeline and cache update logic, you can rewind your Webhook Subscription in the Sequin web console.

You can also pause and resume your Webhook Subscription in the Sequin web console.

## Writing to your cache

Because cache update operations are usually idempotent, you can process batches of rows. If your worker dies mid-way through processing a batch, it's safe for another worker to re-process the same batch from the beginning.

Here's an example using Redis:

```javascript
async function updateCache(rows) {
  // Process one row at a time if your cache doesn't support batch operations
  for (const row of rows) {
    const { id, ...data } = row.record;
    
    // Use SET to update the cache
    // The key is derived from the table's primary key
    await redis.set(
      `users:${id}`,
      JSON.stringify(data),
      // Optional: Set expiration
      'EX', 
      86400 // 24 hours
    );
  }
}
```

For search indexes like Elasticsearch:

```javascript
async function updateCache(rows) {
  for (const row of rows) {
    const { id, ...data } = row.record;
    
    // Index/update document
    await elasticsearch.index({
      index: 'users',
      id: id,
      body: data,
      // Refresh immediately for testing
      refresh: true
    });
  }
}
```

The key principles are:

1. Use the primary key from the source table to generate cache keys
2. Handle one row at a time if batching isn't supported
3. Use operations that are idempotent (SET vs APPEND)

## Deploying to production

Once you've tested your cache maintenance pipeline locally, you can deploy it to production.

You can use the Sequin CLI to export your local configuration and apply it to your production environment:

<Steps>
  <Step title="Perform migrations">
    Ensure your production destination database has the tables you want to replicate to.
    
    We recommend deploying and migrating before proceeding.
  </Step>

  <Step title="Set your Sequin token">
    If you're using a Consumer Group, ensure your Sequin token is configured in your production environment.
  </Step>

  <Step title="Export your configuration">
    After [setting up the Sequin CLI](/cli), export your local configuration:

    ```bash
    sequin config export --context=local
    ```

    This creates a `sequin.yaml` file you can commit to your repository.

    <Info>
      Consider having different YAML files for different environments. For example, `sequin.staging.yaml` and `sequin.production.yaml`.

      Database names often differ between environments (e.g. `my_db_dev` vs `my_db_prod`).
    </Info>
  </Step>

  <Step title="Apply to production">
    Apply your configuration to production:

    ```bash
    sequin config apply sequin.yaml --context=production
    ```
  </Step>
</Steps>

## Re-warming caches

You may need to re-warm your cache in these scenarios:

1. **After infrastructure changes**: Cache server upgrades/migrations
2. **Data model updates**: Schema or business logic changes
3. **Cache eviction**: Unexpected cache flushes

To re-warm your cache, go to your Consumer Group or Webhook Subscription. Click "Rewind". Indicate how far back in time you want to re-warm your cache. You can choose to replay the entire table in the stream, or just rows from the last few days, etc.
