---
title: "How to maintain caches with Sequin"
sidebarTitle: "Maintain caches"
description: "Keep your caches warm and up-to-date using Sequin"
---

This guide shows you how to keep your caches synchronized with your Postgres database using Sequin for change data capture.

By the end, you'll have a pipeline that streams database changes to your caching layer while handling common challenges like cache invalidation, race conditions, and recovery from failures.

## When to use this guide

This approach works well when you need to:
- Keep Redis, Memcached, or other caches in sync with your database
- Maintain search indexes that reflect your current data
- Update materialized views or derived data stores
- Ensure cache consistency across multiple services

## Prerequisites

This guide assumes you have:

1. [Already have Sequin installed](/quickstart/webhooks)
2. [Have a database connected](/quickstart/connect-postgres)
3. A caching system (Redis, Memcached, etc.) ready to receive updates
4. A queue or stream that can receive messages (SQS, Kafka, Redis, etc.) or an HTTP endpoint that can receive webhooks

## Architecture overview

Your cache maintenance pipeline will have these components:

1. **Source table(s)** in Postgres that contain the data you want to cache
2. **Destination sink** (SQS, Kafka, Redis, or webhooks) that delivers changes to your processing system
3. **Cache update processor** that receives changes and updates your cache

## Setup your destination queue or stream

It will be important for your system to process messages in order.

For Kafka, webhooks, and Sequin Stream sinks, you don't need to do anything special. During setup (below), you'll configure Sequin to write messages in order.

For SQS, be sure to use a FIFO queue.

For Redis, messages will be written to your Redis Stream in order. But there's risk you may have race conditions as you process them. See 

DOCSTODO

## Create a sink

First, create a sink to the queue, stream, or webhook endpoint that you want to use to process changes:

<Steps>
  <Step title="Select the source">
    Select the table containing the data you want to cache.

    Optionally add SQL filters to sync a subset of your source table to the destination.
  </Step>

  <Step title="Select the message type">
    Select "changes" as the message type.
  </Step>

  <Step title="Leave message grouping default">
    If your sink supports message grouping, leave the default option selected for "Message grouping".
    
    This will ensure that messages are [grouped by primary key](/reference/ordering-and-grouping), helping eliminate race conditions as you write rows to your cache.
  </Step>

  <Step title="Specify backfill">
    You can optionally backfill your cache with data currently in your source table.

    Backfill messages are [change messages](/reference/messages#change-messages) where the `action` is `read`.
  </Step>

  <Step title="Specify batch size">
    If both your sink and cache support batch operations, consider setting a batch size > `1`. Cache update operations are usually idempotent, so it's safe to process batches. A batch size of 100 is a good place to start.
  </Step>

  <Step title="Configure sink-specific settings">
    Configure sink-specific settings and click "Create Sink".
  </Step>
</Steps>

## Implement cache updates

Here's an example of how you might process changes to update a cache:

```python
def process_change(change):
    # Extract key information
    cache_key = f"{change.metadata.table_name}:{change.record['id']}"
    
    if change.action in ['insert', 'update', 'read']:
        # Optional: Add version/timestamp
        change.record['last_updated'] = change.metadata.commit_timestamp
        
        # Update cache
        cache.set(
            cache_key,
            json.dumps(change.record),
            ex=86400  # Consider your expiry needs
        )
    elif change.action == 'delete':
        cache.delete(cache_key)
```

### Consideration for Redis Streams

If you're using Redis Streams, you'll need to be mindful of race conditions as you process messages.

While Sequin will write messages pertaining to the same row to your stream in order, there's a risk that two separate workers could pull 

DOCSTODO: What's the recommendation? How can we give them an easiy `{commit_lsn, commit_seq}` to use?

## Verify your pipeline is working

If you specified a backfill, there should be messages in your stream ready for your system to process:

1. On the sink overview page, click the "Messages" tab. You should see messages flowing to your sink.
2. Check the logs of your cache update processor to ensure it's processing messages as expected.

## Maintenance

### Re-warming your cache

You may need to re-warm your cache in these scenarios:

1. **After infrastructure changes**: Cache server upgrades/migrations
2. **Data model updates**: Schema or business logic changes
3. **Cache eviction**: Unexpected cache flushes

You can use a [backfill](/reference/backfills) to play `read` operations from your table into your stream.

## Next steps

See "[Deploy to production](/how-to/deploy-to-production)" for guidance on copying your local sink configuration to your production environment.