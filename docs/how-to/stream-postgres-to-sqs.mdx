---
title: "How to stream Postgres to Amazon SQS"
sidebarTitle: "Stream to SQS"
description: "Receive Postgres changes in Amazon SQS in real-time"
---

This guide shows you how to use Sequin to set up real-time streaming of database changes from Postgres to Amazon SQS.

With Postgres data streaming to SQS, you can trigger workflows, keep services in sync, [build audit logs](/how-to/create-audit-logs), [maintain caches](/how-to/maintain-caches), and more.

By the end of this how-to, you'll have database changes flowing to an SQS queue.

## Prerequisites

This guide assumes you:

1. [Already have Sequin installed](/quickstart/webhooks)
2. [Have a database connected](/quickstart/connect-postgres)
3. Have an AWS SQS queue ready to go

## Basic setup

### Create an IAM user for Sequin

First, create an IAM user that Sequin will use to send messages to your SQS queue:

1. Navigate to the IAM service in your AWS Console
2. Click "Users" then "Create user"
3. Name the user (e.g., "sequin-sqs-publisher")
4. Select "Access key - Programmatic access"

<Accordion title="Required IAM Policy">
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Action": [
          "sqs:SendMessage",
          "sqs:SendMessageBatch",
          "sqs:GetQueueAttributes",
          "sqs:GetQueueUrl",
          "sqs:ListQueues"
        ],
        "Resource": "<your-queue-arn>"
      }
    ]
  }
  ```
</Accordion>

Replace `<your-queue-arn>` with your actual queue ARN.

After creating the user, save the Access Key ID and Secret Access Key - you'll need these when configuring the SQS sink in Sequin.

### (Optional) Create a Change Capture Pipeline

You can send either [changes or rows](/reference/messages) to SQS.

If you're sending changes, note that changes are ephemeral. Sequin stores `insert`, `update`, and `delete` changes in a buffer table until they're delivered to SQS.

You can use a [Change Capture Pipeline](/reference/change-capture-pipelines) to persist changes to a table in your database. Then, you can stream _that_ table to SQS. This gives you the power to [run backfills](/reference/backfills)/replays of recent changes at any times. This can be handy: for example, if you realize there's a bug in your SQS message handling implementation, you can deploy the fix and re-process changes from the last few minutes or days.

## Create SQS sink

Navigate to the "Sinks" tab, click "Create Sink", and select "SQS Sink".

### Configure the source

<Steps>
  <Step title="Select source table">
    Under "Source", select the table you want to stream data from.
  </Step>

  <Step title="Choose message type">
    Specify whether you want to receive [changes or rows](/reference/messages) from the table.

    If you're not sure which to choose, select **Changes**. Changes are a better fit for event-driven use cases, like triggering workflows, which is usually what you want to do with SQS.
  </Step>

  <Step title="Specify filters">
    If you selected changes, in "Records to process", you can indicate whether you want to receive `insert`, `update`, and/or `delete` changes.

    You can also specify [SQL filters](/reference/filters) to narrow down the events you want to receive. For example, if you only want to receive events for `subscriptions` that currently have an `mrr` greater than $100, you can add a filter on `mrr > 100`.
  </Step>

  <Step title="Specify backfill">
    You can optionally indicate if you want SQS to receive a [backfill](reference/backfills) of all or a portion of the table's existing data. Backfills are useful if you want SQS to process historical data. For example, if you're materializing a cache, you might want to warm it with existing rows.

    You can backfill at any time. If you don't want to backfill, toggle "Backfill" off.
  </Step>

  <Step title="Specify message grouping">
    Under "Message grouping", you'll most likely want to leave the default option selected to ensure events for the same row are sent to SQS in order.

    That way, if you're using an SQS FIFO queue, your system will receive events for a row in the [same order they occurred in](reference/sqs#message-grouping).
  </Step>

</Steps>

### Configure SQS

Fill in the queue's URL, as well as the AWS client ID and secret you configured for Sequin.

If your queue is a FIFO queue, be sure to include the `.fifo` extension in the queue's URL.

Then, click "Create SQS Sink".

## Verify & debug

To verify that your SQS sink is working:

1. Make some changes in your source table
2. Verify that the count of messages for your sink increases in the Sequin web console
3. In the AWS Console, navigate to your queue. Click "Send and receive messages" then "Poll for messages". You should see the messages from Sequin appear in the table below.

If messages don't seem to be flowing:

1. Click the "Messages" tab to view the state of messages for your sink
2. Click any failed message
3. Check the delivery logs for error details, including any AWS API errors

## Next steps

Assuming you've followed the steps above for your local environment, "[How to deploy to production](/how-to/deploy-to-production)" will show you how to deploy your implementation to production.

For advanced configuration and more about how SQS sinks work, see the [SQS sink reference](/reference/sinks/sqs).
